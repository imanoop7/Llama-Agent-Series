{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-llms-ollama\n",
      "  Downloading llama_index_llms_ollama-0.5.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting llama-index-core<0.13.0,>=0.12.4 (from llama-index-llms-ollama)\n",
      "  Downloading llama_index_core-0.12.10.post1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting ollama>=0.4.3 (from llama-index-llms-ollama)\n",
      "  Downloading ollama-0.4.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting PyYAML>=6.0.1 (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Using cached PyYAML-6.0.2-cp311-cp311-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting SQLAlchemy>=1.4.49 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Using cached SQLAlchemy-2.0.36-cp311-cp311-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.6 (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Using cached aiohttp-3.11.11-cp311-cp311-win_amd64.whl.metadata (8.0 kB)\n",
      "Collecting dataclasses-json (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Using cached Deprecated-1.2.15-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting fsspec>=2023.5.0 (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting httpx (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in d:\\github\\llama-agent-series\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (1.6.0)\n",
      "Collecting networkx>=3.0 (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting nltk>3.8.1 (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting numpy (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Using cached numpy-2.2.1-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Collecting pillow>=9.0.0 (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Using cached pillow-11.1.0-cp311-cp311-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting pydantic>=2.8.0 (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Downloading pydantic-2.10.5-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting requests>=2.31.0 (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.2.0 (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Using cached tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting tiktoken>=0.3.3 (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Using cached tiktoken-0.8.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting tqdm<5.0.0,>=4.66.1 (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in d:\\github\\llama-agent-series\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (4.12.2)\n",
      "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting wrapt (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Using cached wrapt-1.17.0-cp311-cp311-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting httpx (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Using cached httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Using cached attrs-24.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Using cached frozenlist-1.5.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Using cached multidict-6.1.0-cp311-cp311-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Using cached propcache-0.2.1-cp311-cp311-win_amd64.whl.metadata (9.5 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Using cached yarl-1.18.3-cp311-cp311-win_amd64.whl.metadata (71 kB)\n",
      "Collecting anyio (from httpx->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Downloading anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting certifi (from httpx->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Using cached certifi-2024.12.14-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting httpcore==1.* (from httpx->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sniffio (from httpx->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting click (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting joblib (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Using cached regex-2024.11.6-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Using cached pydantic_core-2.27.2-cp311-cp311-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Using cached charset_normalizer-3.4.1-cp311-cp311-win_amd64.whl.metadata (36 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Using cached greenlet-3.1.1-cp311-cp311-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: colorama in d:\\github\\llama-agent-series\\.venv\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (0.4.6)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
      "  Downloading marshmallow-3.24.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: packaging>=17.0 in d:\\github\\llama-agent-series\\.venv\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (24.2)\n",
      "Downloading llama_index_llms_ollama-0.5.0-py3-none-any.whl (6.7 kB)\n",
      "Downloading llama_index_core-0.12.10.post1-py3-none-any.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.6 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.6 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.5/1.6 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 0.8/1.6 MB 958.5 kB/s eta 0:00:01\n",
      "   ------------------- -------------------- 0.8/1.6 MB 958.5 kB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.0/1.6 MB 774.8 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.3/1.6 MB 789.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 837.5 kB/s eta 0:00:00\n",
      "Downloading ollama-0.4.5-py3-none-any.whl (13 kB)\n",
      "Using cached aiohttp-3.11.11-cp311-cp311-win_amd64.whl (442 kB)\n",
      "Using cached Deprecated-1.2.15-py2.py3-none-any.whl (9.9 kB)\n",
      "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Using cached httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached pillow-11.1.0-cp311-cp311-win_amd64.whl (2.6 MB)\n",
      "Downloading pydantic-2.10.5-py3-none-any.whl (431 kB)\n",
      "Using cached pydantic_core-2.27.2-cp311-cp311-win_amd64.whl (2.0 MB)\n",
      "Using cached PyYAML-6.0.2-cp311-cp311-win_amd64.whl (161 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached SQLAlchemy-2.0.36-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "Using cached tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Using cached tiktoken-0.8.0-cp311-cp311-win_amd64.whl (884 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached wrapt-1.17.0-cp311-cp311-win_amd64.whl (38 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached numpy-2.2.1-cp311-cp311-win_amd64.whl (12.9 MB)\n",
      "Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached attrs-24.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached certifi-2024.12.14-py3-none-any.whl (164 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp311-cp311-win_amd64.whl (102 kB)\n",
      "Using cached frozenlist-1.5.0-cp311-cp311-win_amd64.whl (51 kB)\n",
      "Using cached greenlet-3.1.1-cp311-cp311-win_amd64.whl (298 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading marshmallow-3.24.2-py3-none-any.whl (49 kB)\n",
      "Using cached multidict-6.1.0-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Using cached propcache-0.2.1-cp311-cp311-win_amd64.whl (44 kB)\n",
      "Using cached regex-2024.11.6-cp311-cp311-win_amd64.whl (274 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Using cached yarl-1.18.3-cp311-cp311-win_amd64.whl (91 kB)\n",
      "Downloading anyio-4.8.0-py3-none-any.whl (96 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: filetype, dirtyjson, wrapt, urllib3, tqdm, tenacity, sniffio, regex, PyYAML, pydantic-core, propcache, pillow, numpy, networkx, mypy-extensions, multidict, marshmallow, joblib, idna, h11, greenlet, fsspec, frozenlist, click, charset-normalizer, certifi, attrs, annotated-types, aiohappyeyeballs, yarl, typing-inspect, SQLAlchemy, requests, pydantic, nltk, httpcore, deprecated, anyio, aiosignal, tiktoken, httpx, dataclasses-json, aiohttp, ollama, llama-index-core, llama-index-llms-ollama\n",
      "Successfully installed PyYAML-6.0.2 SQLAlchemy-2.0.36 aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 annotated-types-0.7.0 anyio-4.8.0 attrs-24.3.0 certifi-2024.12.14 charset-normalizer-3.4.1 click-8.1.8 dataclasses-json-0.6.7 deprecated-1.2.15 dirtyjson-1.0.8 filetype-1.2.0 frozenlist-1.5.0 fsspec-2024.12.0 greenlet-3.1.1 h11-0.14.0 httpcore-1.0.7 httpx-0.27.2 idna-3.10 joblib-1.4.2 llama-index-core-0.12.10.post1 llama-index-llms-ollama-0.5.0 marshmallow-3.24.2 multidict-6.1.0 mypy-extensions-1.0.0 networkx-3.4.2 nltk-3.9.1 numpy-2.2.1 ollama-0.4.5 pillow-11.1.0 propcache-0.2.1 pydantic-2.10.5 pydantic-core-2.27.2 regex-2024.11.6 requests-2.32.3 sniffio-1.3.1 tenacity-9.0.0 tiktoken-0.8.0 tqdm-4.67.1 typing-inspect-0.9.0 urllib3-2.3.0 wrapt-1.17.0 yarl-1.18.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install llama-index-llms-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply two numbers and returns the product\"\"\"\n",
    "    return a * b\n",
    "\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"Add two numbers and returns the sum\"\"\"\n",
    "    return a + b\n",
    "\n",
    "add_tool = FunctionTool.from_defaults(fn=add)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama3.2\", request_timeout=120.0)\n",
    "agent = ReActAgent.from_tools([multiply_tool, add_tool], llm=llm, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 97782deb-ba37-4ac8-b044-489a4b6c544c. Step input: What is 20+(2*4)? Calculate step by step.\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: add\n",
      "Action Input: {'a': 20, 'b': 8}\n",
      "\u001b[0m\u001b[1;3;34mObservation: 28\n",
      "\u001b[0m> Running step afec744d-3ebf-4055-9b3c-10e9d7eb5dbc. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: The result of the expression 20 + (2 * 4) is 28.\n",
      "\u001b[0mThe result of the expression 20 + (2 * 4) is 28.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"What is 20+(2*4)? Calculate step by step.\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
